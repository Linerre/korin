#+TITLE: Rust Atomic and Locks
#+AUTHOR: Mara Bos

* Schedule
| Chapter | Topic             | Start Date |   Deadline | Status    |
|---------+-------------------+------------+------------+-----------|
|       1 | Basic Concurrency | 2023-11-17 | 2023-11-20 | DONE      |
|       2 | Atomic operations | 2023-11-21 | 2023-11-24 | DONE      |
|       3 | Memory Ordering   | 2023-11-25 | 2023-11-28 | DONE      |
|       4 | Spin Lock         | 2023-11-29 | 2023-12-02 | DONE      |
|       5 | One-shot Channels | 2023-12-03 | 2023-12-06 | DONE      |
|       6 | Implement Arc     | 2023-12-07 | 2023-12-10 | Half DONE |
|       7 | Processors        | 2023-12-11 | 2023-12-14 | DOING     |
|       8 | OS primitives     | 2023-12-15 | 2023-12-18 | TODO      |
|       9 | Build Locks       | 2023-12-19 | 2023-12-22 | TODO      |
|      10 | Ideas/Inspiration | 2023-12-23 | 2023-12-23 | TODO      |
|---------+-------------------+------------+------------+-----------|


Once having learned some basic operating systems, then resume from chapter 6.

* Environment
#+BEGIN_SRC bash
rustc --version
# 1.73.0 (cc66ad468 2023-10-03)

emacs --version
# GNU Emacs 29.0.90
#+END_SRC


* Supplementary
[[file:~/projects/korin/books/rust_in_action/notes.org][Rust in Action]]


* Chapter 01 Basics of Rust Concurrency
** Threads in Rust
In [[https://github.com/m-ou-se/rust-atomics-and-locks/blob/main/examples/ch1-01-hello.rs][Example 01]], when the main fn (in main thread) returns (ends its execution), it shuts down the program regardless of whether there is any other spawned and running thread.

#+BEGIN_SRC rust
// ...
fn main() {
    thread::spawn(f);           // may or may not finish
    thread::spawn(f);

    println!("Hello from the main thread.");
}
// ...
#+END_SRC

Throughout the book, =mian= thread is sometimes referred to as =foreground= thread.  Spawned threads, =background= threads.


** Scoped threads
~std::thread::scope()~, according to Rust's doc, will "create a scope for spawning scoped thread".  I think this means the scope will be running in the environment of main thread, which is the 1st argument passed to the ~scope()~ fn.


** ~Cell~ and ~RefCell~
Useful in a single thread but pretty useless/limited in multi-thread.



** ~Mutex~
Implicitly dropping ~MutexGuard~ is convenient but can lead to a common pitfall
#+BEGIN_SRC rust
// Suppose we have a Mutex<Vec<i32>>
// This line of code takes the Vec and push one item to it and unlock the Mutex
list.lock().unwrap().push(1);
// `MutexGuard` or any temporaries produced will be dropped after `;` which is
// the end of the statement

if let Some(item) = list.lock().unwrap().pop() {// but we want to unlock it right before the block
    process_item(item);
} // MutexGuard will be dropped (unlocked) only at here!

// As boolean value in conditionals does not borrow any data
if list.lock().unwrap().pop() == Some(1) {// unlocked before the block
    do_something();
}
#+END_SRC
Putting a thread to sleep means the thread will not consume any more resources.



* Chapter 02 Atomics
Atomic: indivisible
#+BEGIN_QUOTE
it is either fully completed, or it didn’t happen yet.
#+END_QUOTE
Another way to think about this is "one at a time":
#+BEGIN_QUOTE
Another way to think of an atomic operation is that no observer of an atomic operation can "see" the operation as in-progress. You can observe the operation as not yet started or as completed, but never in between.

For example, when accessing or mutating a property is atomic, it means that only one read or write operation can be performed at a time. If you have a program that reads a property atomically, this means that the property cannot change during this read operation.
#+END_QUOTE
from:[[https://www.donnywals.com/what-does-atomic-mean-in-programming/][What does “atomic” mean in programming?]]

#+BEGIN_QUOTE
Atomicity is usually achieved via "mutual exclusion". That is, there are areas of code that only one thread is allowed to run at a time. If a second thread tries, it has to wait until the first thread is done.
#+END_QUOTE
from:[[https://www.reddit.com/r/learnprogramming/comments/58wz36/c_what_does_atomic_mean/][[C++] What does "atomic" mean?]]


** Atomic Load and Store Operations
*** Lazy Initialization *race* vs *data race*
When multiple threads try to access (and probably modify) the same data, it's hard to tell which thread does what first. Thus such data race leads to /undefined behavior/.
#+BEGIN_QUOTE
... data race, which is undefined behavior and impossible in Rust without using unsafe ...
#+END_QUOTE

While multiple threads may try to do the same thing at the same time.  For example. initializing a variable or reading a value. This is race and there still will be a winner.
#+BEGIN_QUOTE
Since we expect x to be constant, it doesn’t matter who wins the race, as the result will be the same regardless.
#+END_QUOTE

Read the doc of Rust ~std::sync::Once~ and ~std::sync::OnceLock~


** Fetch-and-Modify Operations
#+BEGIN_QUOTE
An important thing to keep in mind is that fetch_add and fetch_sub implement /wrapping/ behavior for overflows. Incrementing a value past the maximum representable value will wrap around and result in the minimum representable value.
#+END_QUOTE


* Chapter 03 Memory Ordering
Memory ordering is about how data is stored and accessed in memory.  By "how" I mean the order in which data is written (loaded) to memory and read (released) from it.

On the other hand, compiler and processor often "optimizes" my program by *re-ordering* operations.  Sometimes they are right and much smarter than me.  Sometimes, however, I need to tell them exactly "how".  This chapter is all about /how/ I can tell the compiler/processor about the order of my code.

** Reordering and Optimization
~Relaxed~ is mostly discussed in this section.  It indicates non-strict memory ordering.

#+BEGIN_QUOTE
The logic for verifying that a specific reordering or other optimization won’t affect the behavior of your program does not take other threads into account.
#+END_QUOTE


#+BEGIN_QUOTE
Rust’s memory model, which is /mostly copied from C++/, *doesn’t match any existing processor architecture*, but instead is an abstract model with a strict set of rules that attempt to represent the greatest common denominator of all current and future architectures, while also giving the compiler enough freedom to make useful assumptions while analyzing and optimizing programs.
#+END_QUOTE


** Happens-before relation
#+BEGIN_EXAMPLE
// statements ensured to happen before spawn()
statement 1---------
statement 2         |  Happens-before relationship
thread::spawn()-----

// join() ensured to happen before statements
thread.join()-------
statement 1         |  Happens-before relationship
statement 2---------
#+END_EXAMPLE


** Release and Acquire Ordering
#+BEGIN_QUOTE
A happens-before relationship is formed when an acquire-load operation observes the result of a release-store operation. In this case, the store and everything before it, happened before the load and everything after it.
#+END_QUOTE

#+BEGIN_EXAMPLE
statement 1---------|
statement 2         |
release   ----------|  Happens-before relationship
acquire   ----------|
statement 3         |
statement 4---------
#+END_EXAMPLE
See: [[https://marabos.nl/atomics/memory-ordering.html#happens-before-diagram-release-acquire][Figure 3-3]]


#+BEGIN_QUOTE
one thread releases data by atomically storing some value to an atomic variable, and another thread acquires it by atomically loading that value. This is exactly what happens when we unlock (release) a mutex and subsequently lock (acquire) it on another thread.
#+END_QUOTE


*** pointer
~&*ptr~ in Rust means a *reference* to the data a pointer ~ptr~ points to


*** Further readings
[[https://preshing.com/20120913/acquire-and-release-semantics/][Acquire and Release Semantics]] talks about similar memory sharing in C++11.  "Acquire and Release" is one crucial way for lock-free multi-threads programming.


** Consume Ordering
#+BEGIN_QUOTE
It’s possible that a workable definition and implementation of consume ordering might be found in the future. Until that time arrives, however, Rust does not expose ~Ordering::Consume~.
#+END_QUOTE


** Sequentially Consistent Ordering
Stronger but less frequently used.
#+BEGIN_QUOTE
While it might seem like the easiest memory ordering to reason about, SeqCst ordering is almost never necessary in practice. In nearly all cases, regular acquire and release ordering suffice.
#+END_QUOTE


** Fences
A good read from [[https://doc.rust-lang.org/std/sync/atomic/fn.fence.html][~std::sync::atomic::fence~]]

A "fence", like the word's meaning, works in a way that it separate what it fences from others in memory ordering, thus:
#+BEGIN_EXAMPLE
fence(Release) ---- |
x.store(1, Relaxed) |
                    |
                    | if x(t2) loads stuff from x(t1), Release happens before Acquire
                    |
x.load(1, Relaxed)  |
fence(Acqure) ------
#+END_EXAMPLE

~fence(Acquire)~ alone seems to be used more often:
#+BEGIN_SRC rust
// ....
fence(Acquire);
// do stuff directly with atomic variables here
#+END_SRC


* Chapter 06 Implementing Our Own Arc

* Misc
#+BEGIN_QUOTE
...the memory model doesn’t say anything about timing at all. It only defines in which order certain things happen; not how long you might have to wait for them.

While ~SeqCst~ can stand in for ~Acquire~ or ~Release~, it is not a way to somehow create an acquire-store or release-load. Those remain /nonexistent/. Release only applies to store operations, and acquire only to load operations.
#+END_QUOTE


#+BEGIN_SRC rust
// compare current state with `EMPTY`, if it is `EMPTY`,
// exchange `EMPTY` with `WRITING` (new state)
if self.state.compare_exchange(
    EMPTY, WRITING, Relaxed, Relaxed
).is_err() {
    panic!("can't send more than one message!");
}

// compare current state with `READY`, if it is `READY`,
// exchange `READY` with `READING` (new state)
if self.state.compare_exchange(
    READY, READING, Acquire, Relaxed
).is_err() {
    panic!("no message available!");
}
#+END_SRC

When a fn takes a type (not a reference to the type) as its argument, the fn assumes the type and thus can be called only once.  See the channels design in Chapter 5.

** Technique to read asynchronous/multithreading code
#+BEGIN_SRC rust
fn main() {
    thread::scope(|s| {
        let (sender, receiver) = channel(); // 1
        let t = thread::current();          // 2
        s.spawn(move || {                   // 3-a
            sender.send("hello world!");    // 5
            t.unpark();                     // 6
        });
        while !receiver.is_ready() {        // 3-b
            thread::park();                 // 4
        }
        assert_eq!(receiver.receive(), "hello world!"); // 7
    });
}

/*
 ,* 3-a and 3-b happens at almost the same time.  In other words, thread `t`, the
 ,* current thread, the main thread or the foreground thread, will NOT wait for 3-a
 ,* to finish, but instead directly jumps to 3-b to keep the execution.
 ,* Thus, a better way to read the above code is to follow a specific thread.  In
 ,* case, follow the main thread: 1->2->3-b->4: if msg not read, I sleep
 ,* then, follow the background thread: 3-a->5->6: send msg and kick up the main t
 ,*/

#+END_SRC


** Quotes
#+BEGIN_QUOTE
Having to make trade-offs between safety, convenience, flexibility, simplicity, and performance is unfortunate, but sometimes unavoidable. Rust generally strives to make it easy to excel at all of these, but sometimes makes you trade a bit of one to maximize another.

Chapter 05 Channels: Safety Through Types
#+END_QUOTE
